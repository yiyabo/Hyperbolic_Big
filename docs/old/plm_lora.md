以下是“**家族条件 LoRA**”的**完整方案说明**，按工程可落地的规范来写。你可以把它当作你主方案中的一个独立模块（Plug-in），直接纳入你的 md 文档。

---

# 家族条件 LoRA（Family-Conditioned LoRA）设计规范 v1.0

**目的**：在不引入大型 MoE 的前提下，为不同**家族/功能亚型**提供轻量的“专才化”能力，提升长尾家族与跨物种泛化，同时与主干 **PLM +（RAG）+ 图小波 × 洛伦兹 GNN（可学习曲率）**无缝对接。

---

## 1. 动机与总体思路

* PPI 预测涉及 **家族层次结构**与**功能亚型差异**。单一参数化易被头部家族主导，尾部家族欠拟合。
* **LoRA** 在注意力/FFN 线性层上以**低秩增量**提供可学习自由度；我们让这部分**受“家族条件信号”驱动**，实现**软专家化**而无路由/均衡成本。
* 与 MoE 相比：**参数更小、延迟更稳、实现更简单**；与 Adapter 相比：**适配注意力/FFN 的张量更直接**。

导致它们在生物体中的功能微有不同，比如酶的底物特异性、信号通路的上下游分工、界面结构的不同等。
---

## 2. 条件信号 $z$（输入与构成）

**目标**：用一个向量 $z \in \mathbb{R}^{p}$ 表达“家族上下文”。

* **基础来源**：

  * InterPro/Pfam 的**one-hot** 或 **概率分布**（推荐概率分布，容忍噪声与多标签）。
  * 层级信息（如 Pfam clan/GO term）→ **嵌入**后拼接。
* **可选增强**（与 RAG 协同）：

  * 同源检索统计（top-K 家族/物种频率、保守位点比例等）→ 压缩为 16–32 维向量拼接。
* **预处理**：

  * **温度平滑**：$\tilde{p}=\text{softmax}(\log p/\tau), \tau\!\in[1.5,3]$；
  * **降维**：小 MLP 或线性到 64–128 维；
  * **缺失/未知家族**：回退到“**全局条件**”（零向量或 learned [UNK] 条件）。

---

## 3. 架构选择（从轻到强的 3 个变体）

### 3.1 变体 A：**条件缩放 LoRA（FiLM-α）**【默认起步】

在原线性层 $y = W x$ 上加 LoRA：$\Delta W = BA$（rank $r \ll d$）。
让缩放系数**由条件向量生成**：$\alpha=\alpha(z)$。

$$
y = W x \;+\; \alpha(z)\cdot B(Ax)
$$

* **参数**：主干冻结，仅新增 $(A,B)$ 与很小的 $\text{MLP}_\alpha$。
* **放置**：优先在 PLM 顶部 **$W_q, W_v$** 与 FFN 的 in/out 投影；图侧（洛伦兹 GNN）可在**消息传递线性层**与**读出层**各放 1 个。
* **优点**：极轻量、稳定；能立刻验证“家族条件是否有收益”。

### 3.2 变体 B：**基混合 LoRA（Mixture-of-Bases LoRA）**

预置 $M$ 组 LoRA 基 $\{(A_m,B_m)\}_{m=1}^M$，条件网络输出权重 $w=\text{softmax}(g(z))$：

$$
\Delta W(z) \;=\; \sum_{m=1}^{M} w_m(z)\, B_m A_m,\qquad
y = W x + \Delta W(z)\,x
$$

* **参数**：相当于 $M$ 套 rank-$r$ LoRA（共享跨家族），外加小门控网络 $g$。
* **优点**：表达力显著提升；对**家族多且相关性强**的场景更稳。
* **建议**：$M=4\text{–}8,\ r=4\text{–}8$。

### 3.3 变体 C：**超网络 LoRA（HyperLoRA）**

由条件向量直接**生成** $A(z),B(z)$ 或其部分行/列：

$$
\Delta W(z)=B(z)\,A(z)
$$

* **优点**：最灵活；**缺点**：需更强正则，训练不当易过拟合。
* **使用时机**：当 B 无法拟合复杂跨家族差异再考虑 C。

> **推荐路径**：A 起步 → 若收益明显再切 B；仅当 B 不足再上 C。

---

## 4. 插入位置与形状

* **PLM 侧（ESM-2/3）**

  * 首选：最后 **4–6 层** 的 $W_q, W_v$（注意力）与 FFN（in/out），rank $r=8$（注意力）/ $r=4$（FFN）。
  * 若显存紧张：只在 $W_v$+FFN-out 放置。
* **图侧（洛伦兹 GNN）**

  * 在**消息传递线性层**与**读出层**前放 LoRA/Adapter；**不要**嵌入到 exp/log 映射中，以免数值不稳。
* **图小波接口**

  * 将**图小波输出**与 PLM/RAG 表征拼接后，再经过**家族条件 LoRA** 的线性层，有助于“尺度×家族”的交互。

---

## 5. 训练目标与正则

* **主损失**：PPI 边分类 BCE / 多标签 CE（主指标 AUPR）。
* **门控熵正则**（B/C 变体）：$\mathcal{L}_\text{ent} = -\lambda_{\text{ent}}\sum w_m \log w_m$（防止塌缩）。
* **基正交正则**（B 变体）：$\mathcal{L}_\text{orth} = \lambda_{\text{orth}}\big(\|A^\top A - I\|_F^2 + \|B B^\top - I\|_F^2\big)$。
* **条件平滑**：对家族分布使用温度label smoothing，$\tau \in [1.5,3]$。
* **类不平衡**：可用 focal-$\gamma=2$ 或 class-balanced reweight。
* **（可选）蒸馏**：从 **PSSM-teacher** 或 **无条件模型**蒸馏到家族条件模型（KL/Logit-MSE）。

---

## 6. 训练流程（两阶段）

**阶段 I（快速收敛）**

* 冻结：ESM 主干与洛伦兹几何块的大部分参数；
* 训练：家族条件 LoRA + 读出层（以及图侧小 Adapter）；
* 优化：AdamW（lr=2e-4；wd=0.01；bf16/amp）；梯度裁剪 1.0；Batch 依显存定。

**阶段 II（细化）**

* 解冻：ESM 顶部 2–4 层 LN/FFN，洛伦兹 GNN 的最后一层；
* 调 lr：主干 2e-5、LoRA/读出 1e-4；保持正则与早停；
* 若用 B 变体：开启 $\lambda_\text{ent}\in[5e\!-\!4,2e\!-\!3]$、$\lambda_\text{orth}\in[1e\!-\!4,1e\!-\!3]$。

---

## 7. 推理与部署

* **延迟/显存**：与“无条件 LoRA”几乎一致；A 变体几乎无额外开销。
* **可合并**：需要时可将 LoRA 合并进 $W$（近似或离线 merge）。
* **回退策略**：未识别家族→使用 [UNK] 条件或将 $\alpha(z)=\alpha_0$、$w(z)=$ 均匀分布。
* **A/B**：可在线切换“无条件 vs 家族条件”以验证真实收益。

---

## 8. 复杂度与参数开销估算

对每个线性层（输入维 $d_\text{in}$、输出维 $d_\text{out}$）：

* **LoRA 参数**：$\approx r(d_\text{in}+d_\text{out})$。
* **A 变体额外**：$\text{MLP}_\alpha$（几千～几万参数级）。
* **B 变体额外**：乘以 $M$（共享跨家族，不随家族数线性增长）。
* **与 MoE 对比**：无路由/均衡损失，吞吐稳定；总参数通常远小于“专家数×层数×维度”的 MoE。

---

## 9. 超参数与默认配置（建议）

| 项                        |                     默认 | 备注                            |
| ------------------------ | ---------------------: | ----------------------------- |
| LoRA rank（attn / FFN）    |                  8 / 4 | 顶层优先                          |
| 变体                       |      A 起步，收益稳后试 B（M=8） | C 仅在 B 不足时                    |
| α 生成 MLP 隐层              |                    128 | ReLU + LayerNorm              |
| 门控熵 $\lambda_\text{ent}$ |                   1e-3 | B/C 需要                        |
| 正交 $\lambda_\text{orth}$ |                   5e-4 | B 需要                          |
| 条件向量维度 $p$               |                 64–128 | InterPro/Pfam + clan + RAG 统计 |
| 温度 $\tau$                |                    2.0 | 平滑家族分布                        |
| 解冻层数                     | ESM 顶 2–4 层 + GNN 最后一层 | 阶段 II                         |
| 学习率（LoRA/头 / 主干）         |            2e-4 / 2e-5 | AdamW, wd=0.01                |
| 梯度裁剪                     |                    1.0 | 数值稳定                          |

---

## 10. 与 RAG / 图小波 / 洛伦兹 GNN 的对接

* **RAG**：在训练与推理中都保持 RAG 打开，确保 LoRA 学到“带检索”的分布；同源统计可并入 $z$。
* **图小波**：将多尺度小波特征与 PLM/RAG 表征拼接，再经带 LoRA 的线性层进入洛伦兹 GNN，使“尺度×家族”在同一层发生交互。
* **洛伦兹 GNN**：家族条件 LoRA 放在线性消息/读出层，**不要**改动 exp/log 映射与曲率更新，避免破坏几何一致性。

---

## 11. 评测与消融（必须报告）

1. **无条件 vs 家族条件（A/B）**：整体与**尾部家族**分组的 AUPR/Rec@K。
2. **家族 OOD（冷启动）**：训练未见家族 → 使用 [UNK] 或最近邻 clan 初始化，比较增益。
3. **尺度×家族交互**：不同小波尺度下的家族增益热图。
4. **几何交互**：洛伦兹半径/曲率与家族条件权重的相关图。
5. **RAG 协同**：关闭/开启 RAG 的对照 + K-sensitivity。
6. **可解释性**：输出 $\alpha(z)$ 或 $w(z)$ 分布、删失实验（置零条件向量的 ΔAUPR）。

---

## 12. 常见坑与规避

* **家族标签噪声**：用 $\tau$ 平滑 + “回退专家”；必要时对条件向量做 dropout。
* **长尾过拟合**：rank/瓶颈取小，正则/早停；使用 class-balanced loss。
* **数值不稳**：避免在几何映射处插 LoRA；全局开启 grad clip。
* **门控塌缩**（B 变体）：提高 $\lambda_\text{ent}$ 或增大 $M$；引入最小使用率约束。
* **推理漂移**：确保训练/推理 InterPro 版本一致；未知家族统一回退策略。

---

### 一句话总结

> **家族条件 LoRA** = 在 ESM（及图侧）关键线性层引入**低秩增量**，并让其**由家族条件向量调制**（缩放/混合/生成）。它以**极低参数与稳定延迟**换来“针对家族的软专才化”，与 **RAG、图小波、洛伦兹几何**互补，是你“**尺度—家族—几何**”主线中最具性价比的专才化实现。
