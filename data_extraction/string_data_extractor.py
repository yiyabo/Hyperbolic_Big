#!/usr/bin/env python3
"""
STRINGæ•°æ®åº“æå–å™¨
ä»STRING v12.0æ•°æ®åº“æå–ç½®ä¿¡åº¦>0.95çš„è·¨ç‰©ç§è›‹ç™½è´¨ç›¸äº’ä½œç”¨æ•°æ®
"""

import requests
import gzip
from pathlib import Path
import logging
from typing import Dict
from tqdm import tqdm
import sqlite3

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class StringDataExtractor:
    """STRINGæ•°æ®æå–å™¨"""
    
    def __init__(self, data_dir: str = "data", confidence_threshold: float = 0.95):
        """
        åˆå§‹åŒ–æå–å™¨
        
        Args:
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        self.confidence_threshold = confidence_threshold
        
        # STRING v12.0 ä¸‹è½½URLs - ä¸ºå±‚æ¬¡åŒ–ç‰¹å¾å»ºæ¨¡æ‰©å±•
        self.urls = {
            # åŸºç¡€æ•°æ®
            'protein_links': 'https://stringdb-downloads.org/download/protein.links.v12.0.txt.gz',
            'protein_info': 'https://stringdb-downloads.org/download/protein.info.v12.0.txt.gz', 
            'protein_sequences': 'https://stringdb-downloads.org/download/protein.sequences.v12.0.fa.gz',
            
            # å±‚æ¬¡èšç±»æ•°æ®ï¼ˆç”¨äºMoEä¸“å®¶æ¨¡å‹çš„å®¶æ—åˆ†ç±»ï¼‰
            'clusters_proteins': 'https://stringdb-downloads.org/download/clusters.proteins.v12.0.txt.gz',
            'clusters_info': 'https://stringdb-downloads.org/download/clusters.info.v12.0.txt.gz',
            'clusters_tree': 'https://stringdb-downloads.org/download/clusters.tree.v12.0.txt.gz',
            
            # è¯¦ç»†ç›¸äº’ä½œç”¨æ•°æ®ï¼ˆå¤šé€šé“è¯æ®è¯„åˆ†ï¼‰
            'protein_links_detailed': 'https://stringdb-downloads.org/download/protein.links.detailed.v12.0.txt.gz'
        }
        
        # æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        self.db_path = self.data_dir / "string_data.db"
        
    def download_file(self, url: str, filename: str) -> Path:
        """ä¸‹è½½å¹¶è§£å‹æ–‡ä»¶"""
        file_path = self.data_dir / filename
        
        if file_path.exists():
            logger.info("æ–‡ä»¶ %s å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½", filename)
            return file_path
            
        logger.info("å¼€å§‹ä¸‹è½½ %s...", filename)
        response = requests.get(url, stream=True, timeout=300)
        response.raise_for_status()
        
        # è·å–æ–‡ä»¶å¤§å°
        total_size = int(response.headers.get('content-length', 0))
        
        with open(file_path, 'wb') as f:
            with tqdm(total=total_size, unit='B', unit_scale=True) as pbar:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
                    pbar.update(len(chunk))
        
        logger.info("ä¸‹è½½å®Œæˆ: %s", filename)
        return file_path
    
    def extract_gz_file(self, gz_path: Path) -> Path:
        """è§£å‹.gzæ–‡ä»¶"""
        output_path = gz_path.with_suffix('')
        
        if output_path.exists():
            logger.info("è§£å‹æ–‡ä»¶ %s å·²å­˜åœ¨ï¼Œè·³è¿‡è§£å‹", output_path.name)
            return output_path
            
        logger.info("è§£å‹æ–‡ä»¶: %s", gz_path.name)
        with gzip.open(gz_path, 'rb') as f_in:
            with open(output_path, 'wb') as f_out:
                f_out.write(f_in.read())
        
        logger.info("è§£å‹å®Œæˆ: %s", output_path.name)
        return output_path
    
    def setup_database(self):
        """åˆ›å»ºSQLiteæ•°æ®åº“å’Œè¡¨ç»“æ„"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºè›‹ç™½è´¨ä¿¡æ¯è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS protein_info (
                protein_id TEXT PRIMARY KEY,
                protein_name TEXT,
                species_id INTEGER,
                species_name TEXT,
                annotation TEXT
            )
        ''')
        
        # åˆ›å»ºè›‹ç™½è´¨ç›¸äº’ä½œç”¨è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS protein_interactions (
                protein1 TEXT,
                protein2 TEXT,
                combined_score INTEGER,
                PRIMARY KEY (protein1, protein2)
            )
        ''')
        
        # åˆ›å»ºè¯¦ç»†ç›¸äº’ä½œç”¨è¡¨ï¼ˆå¤šé€šé“è¯æ®è¯„åˆ†ï¼‰
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS protein_interactions_detailed (
                protein1 TEXT,
                protein2 TEXT,
                neighborhood REAL,
                fusion REAL,
                cooccurence REAL,
                coexpression REAL,
                experimental REAL,
                database REAL,
                textmining REAL,
                combined_score REAL,
                PRIMARY KEY (protein1, protein2)
            )
        ''')
        
        # åˆ›å»ºè›‹ç™½è´¨åºåˆ—è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS protein_sequences (
                protein_id TEXT PRIMARY KEY,
                sequence TEXT
            )
        ''')
        
        # åˆ›å»ºèšç±»ä¿¡æ¯è¡¨ï¼ˆç”¨äºMoEä¸“å®¶æ¨¡å‹çš„å®¶æ—åˆ†ç±»ï¼‰
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS cluster_info (
                cluster_id TEXT PRIMARY KEY,
                cluster_name TEXT,
                cluster_description TEXT,
                cluster_size INTEGER
            )
        ''')
        
        # åˆ›å»ºè›‹ç™½è´¨èšç±»æ˜ å°„è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS protein_clusters (
                protein_id TEXT,
                cluster_id TEXT,
                PRIMARY KEY (protein_id, cluster_id)
            )
        ''')
        
        # åˆ›å»ºèšç±»å±‚æ¬¡æ ‘è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS cluster_tree (
                child_cluster_id TEXT,
                parent_cluster_id TEXT,
                distance REAL,
                PRIMARY KEY (child_cluster_id, parent_cluster_id)
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_species ON protein_info(species_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_score ON protein_interactions(combined_score)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_detailed_score ON protein_interactions_detailed(combined_score)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_cluster ON protein_clusters(cluster_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_protein_cluster ON protein_clusters(protein_id)')
        
        conn.commit()
        conn.close()
        logger.info("æ•°æ®åº“è¡¨ç»“æ„åˆ›å»ºå®Œæˆ")
    
    def load_protein_info(self):
        """åŠ è½½è›‹ç™½è´¨ä¿¡æ¯åˆ°æ•°æ®åº“"""
        # ä¸‹è½½å¹¶è§£å‹è›‹ç™½è´¨ä¿¡æ¯æ–‡ä»¶
        gz_path = self.download_file(self.urls['protein_info'], 'protein.info.v12.0.txt.gz')
        txt_path = self.extract_gz_file(gz_path)
        
        logger.info("å¼€å§‹åŠ è½½è›‹ç™½è´¨ä¿¡æ¯...")
        conn = sqlite3.connect(self.db_path)
        
        # æ‰¹é‡æ’å…¥æ•°æ®
        chunk_size = 10000
        chunks_processed = 0
        
        with open(txt_path, 'r', encoding='utf-8') as f:
            # è·³è¿‡å¤´éƒ¨
            next(f)
            
            batch_data = []
            for line in tqdm(f, desc="å¤„ç†è›‹ç™½è´¨ä¿¡æ¯"):
                parts = line.strip().split('\t')
                if len(parts) >= 3:
                    protein_id = parts[0]
                    protein_name = parts[1]
                    species_id = int(protein_id.split('.')[0])
                    annotation = parts[2] if len(parts) > 2 else ""
                    
                    batch_data.append((protein_id, protein_name, species_id, "", annotation))
                    
                    if len(batch_data) >= chunk_size:
                        conn.executemany(
                            'INSERT OR REPLACE INTO protein_info VALUES (?, ?, ?, ?, ?)',
                            batch_data
                        )
                        conn.commit()
                        batch_data = []
                        chunks_processed += 1
            
            # æ’å…¥å‰©ä½™æ•°æ®
            if batch_data:
                conn.executemany(
                    'INSERT OR REPLACE INTO protein_info VALUES (?, ?, ?, ?, ?)',
                    batch_data
                )
                conn.commit()
        
        conn.close()
        logger.info("è›‹ç™½è´¨ä¿¡æ¯åŠ è½½å®Œæˆï¼Œå¤„ç†äº† %d ä¸ªæ‰¹æ¬¡", chunks_processed)
    
    def filter_high_confidence_interactions(self):
        """ç­›é€‰é«˜ç½®ä¿¡åº¦çš„è›‹ç™½è´¨ç›¸äº’ä½œç”¨"""
        # ä¸‹è½½å¹¶è§£å‹ç›¸äº’ä½œç”¨æ–‡ä»¶
        gz_path = self.download_file(self.urls['protein_links'], 'protein.links.v12.0.txt.gz')
        txt_path = self.extract_gz_file(gz_path)
        
        logger.info("å¼€å§‹ç­›é€‰ç½®ä¿¡åº¦ > %.2f çš„ç›¸äº’ä½œç”¨...", self.confidence_threshold)
        conn = sqlite3.connect(self.db_path)
        
        chunk_size = 10000
        high_confidence_count = 0
        total_count = 0
        
        with open(txt_path, 'r', encoding='utf-8') as f:
            # è·³è¿‡å¤´éƒ¨
            next(f)
            
            batch_data = []
            for line in tqdm(f, desc="ç­›é€‰é«˜ç½®ä¿¡åº¦ç›¸äº’ä½œç”¨"):
                parts = line.strip().split()
                if len(parts) >= 3:
                    protein1 = parts[0]
                    protein2 = parts[1]
                    combined_score = int(parts[2])
                    
                    total_count += 1
                    
                    # åªä¿ç•™é«˜ç½®ä¿¡åº¦çš„ç›¸äº’ä½œç”¨
                    if combined_score >= self.confidence_threshold * 1000:  # STRINGåˆ†æ•°æ˜¯0-1000
                        batch_data.append((protein1, protein2, combined_score))
                        high_confidence_count += 1
                        
                        if len(batch_data) >= chunk_size:
                            conn.executemany(
                                'INSERT OR REPLACE INTO protein_interactions VALUES (?, ?, ?)',
                                batch_data
                            )
                            conn.commit()
                            batch_data = []
            
            # æ’å…¥å‰©ä½™æ•°æ®
            if batch_data:
                conn.executemany(
                    'INSERT OR REPLACE INTO protein_interactions VALUES (?, ?, ?)',
                    batch_data
                )
                conn.commit()
        
        conn.close()
        logger.info("ç›¸äº’ä½œç”¨ç­›é€‰å®Œæˆï¼š%d/%d (%.2f%%) ç¬¦åˆé˜ˆå€¼", 
                   high_confidence_count, total_count, high_confidence_count/total_count*100)
    
    def load_protein_sequences(self):
        """åŠ è½½è›‹ç™½è´¨åºåˆ—"""
        # ä¸‹è½½å¹¶è§£å‹åºåˆ—æ–‡ä»¶
        gz_path = self.download_file(self.urls['protein_sequences'], 'protein.sequences.v12.0.fa.gz')
        
        logger.info("å¼€å§‹åŠ è½½è›‹ç™½è´¨åºåˆ—...")
        conn = sqlite3.connect(self.db_path)
        
        chunk_size = 1000
        batch_data = []
        
        with gzip.open(gz_path, 'rt') as f:
            protein_id = None
            sequence = []
            
            for line in tqdm(f, desc="å¤„ç†è›‹ç™½è´¨åºåˆ—"):
                line = line.strip()
                if line.startswith('>'):
                    # ä¿å­˜å‰ä¸€ä¸ªè›‹ç™½è´¨çš„åºåˆ—
                    if protein_id and sequence:
                        batch_data.append((protein_id, ''.join(sequence)))
                        
                        if len(batch_data) >= chunk_size:
                            conn.executemany(
                                'INSERT OR REPLACE INTO protein_sequences VALUES (?, ?)',
                                batch_data
                            )
                            conn.commit()
                            batch_data = []
                    
                    # è§£ææ–°çš„è›‹ç™½è´¨ID
                    protein_id = line[1:].split()[0]
                    sequence = []
                else:
                    sequence.append(line)
            
            # å¤„ç†æœ€åä¸€ä¸ªè›‹ç™½è´¨
            if protein_id and sequence:
                batch_data.append((protein_id, ''.join(sequence)))
            
            # æ’å…¥å‰©ä½™æ•°æ®
            if batch_data:
                conn.executemany(
                    'INSERT OR REPLACE INTO protein_sequences VALUES (?, ?)',
                    batch_data
                )
                conn.commit()
        
        conn.close()
        logger.info("è›‹ç™½è´¨åºåˆ—åŠ è½½å®Œæˆ")
    
    def load_detailed_interactions(self):
        """åŠ è½½è¯¦ç»†çš„è›‹ç™½è´¨ç›¸äº’ä½œç”¨æ•°æ®ï¼ˆå¤šé€šé“è¯æ®è¯„åˆ†ï¼‰"""
        # ä¸‹è½½å¹¶è§£å‹è¯¦ç»†ç›¸äº’ä½œç”¨æ–‡ä»¶
        gz_path = self.download_file(self.urls['protein_links_detailed'], 'protein.links.detailed.v12.0.txt.gz')
        txt_path = self.extract_gz_file(gz_path)
        
        logger.info("å¼€å§‹åŠ è½½è¯¦ç»†ç›¸äº’ä½œç”¨æ•°æ®...")
        conn = sqlite3.connect(self.db_path)
        
        chunk_size = 10000
        batch_data = []
        
        with open(txt_path, 'r', encoding='utf-8') as f:
            # è·³è¿‡å¤´éƒ¨
            next(f)
            
            for line in tqdm(f, desc="å¤„ç†è¯¦ç»†ç›¸äº’ä½œç”¨æ•°æ®"):
                parts = line.strip().split()
                if len(parts) >= 9:
                    protein1 = parts[0]
                    protein2 = parts[1]
                    neighborhood = float(parts[2]) / 1000.0  # è½¬æ¢ä¸º0-1èŒƒå›´
                    fusion = float(parts[3]) / 1000.0
                    cooccurence = float(parts[4]) / 1000.0
                    coexpression = float(parts[5]) / 1000.0
                    experimental = float(parts[6]) / 1000.0
                    database = float(parts[7]) / 1000.0
                    textmining = float(parts[8]) / 1000.0
                    combined_score = float(parts[9]) / 1000.0
                    
                    # åªä¿ç•™é«˜ç½®ä¿¡åº¦çš„ç›¸äº’ä½œç”¨
                    if combined_score >= self.confidence_threshold:
                        batch_data.append((
                            protein1, protein2, neighborhood, fusion, cooccurence,
                            coexpression, experimental, database, textmining, combined_score
                        ))
                        
                        if len(batch_data) >= chunk_size:
                            conn.executemany('''
                                INSERT OR REPLACE INTO protein_interactions_detailed 
                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                            ''', batch_data)
                            conn.commit()
                            batch_data = []
            
            # æ’å…¥å‰©ä½™æ•°æ®
            if batch_data:
                conn.executemany('''
                    INSERT OR REPLACE INTO protein_interactions_detailed 
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', batch_data)
                conn.commit()
        
        conn.close()
        logger.info("è¯¦ç»†ç›¸äº’ä½œç”¨æ•°æ®åŠ è½½å®Œæˆ")
    
    def load_cluster_info(self):
        """åŠ è½½èšç±»ä¿¡æ¯æ•°æ®"""
        # ä¸‹è½½å¹¶è§£å‹èšç±»ä¿¡æ¯æ–‡ä»¶
        gz_path = self.download_file(self.urls['clusters_info'], 'clusters.info.v12.0.txt.gz')
        txt_path = self.extract_gz_file(gz_path)
        
        logger.info("å¼€å§‹åŠ è½½èšç±»ä¿¡æ¯...")
        conn = sqlite3.connect(self.db_path)
        
        chunk_size = 10000
        batch_data = []
        
        with open(txt_path, 'r', encoding='utf-8') as f:
            # è·³è¿‡å¤´éƒ¨
            next(f)
            
            for line in tqdm(f, desc="å¤„ç†èšç±»ä¿¡æ¯"):
                parts = line.strip().split('\t')
                if len(parts) >= 3:
                    cluster_id = parts[0]
                    cluster_name = parts[1] if len(parts) > 1 else ""
                    cluster_description = parts[2] if len(parts) > 2 else ""
                    cluster_size = int(parts[3]) if len(parts) > 3 else 0
                    
                    batch_data.append((cluster_id, cluster_name, cluster_description, cluster_size))
                    
                    if len(batch_data) >= chunk_size:
                        conn.executemany(
                            'INSERT OR REPLACE INTO cluster_info VALUES (?, ?, ?, ?)',
                            batch_data
                        )
                        conn.commit()
                        batch_data = []
            
            # æ’å…¥å‰©ä½™æ•°æ®
            if batch_data:
                conn.executemany(
                    'INSERT OR REPLACE INTO cluster_info VALUES (?, ?, ?, ?)',
                    batch_data
                )
                conn.commit()
        
        conn.close()
        logger.info("èšç±»ä¿¡æ¯åŠ è½½å®Œæˆ")
    
    def load_protein_clusters(self):
        """åŠ è½½è›‹ç™½è´¨èšç±»æ˜ å°„æ•°æ®"""
        # ä¸‹è½½å¹¶è§£å‹è›‹ç™½è´¨èšç±»æ–‡ä»¶
        gz_path = self.download_file(self.urls['clusters_proteins'], 'clusters.proteins.v12.0.txt.gz')
        txt_path = self.extract_gz_file(gz_path)
        
        logger.info("å¼€å§‹åŠ è½½è›‹ç™½è´¨èšç±»æ˜ å°„...")
        conn = sqlite3.connect(self.db_path)
        
        chunk_size = 10000
        batch_data = []
        
        with open(txt_path, 'r', encoding='utf-8') as f:
            # è·³è¿‡å¤´éƒ¨
            next(f)
            
            for line in tqdm(f, desc="å¤„ç†è›‹ç™½è´¨èšç±»æ˜ å°„"):
                parts = line.strip().split('\t')
                if len(parts) >= 2:
                    cluster_id = parts[0]
                    protein_id = parts[1]
                    
                    batch_data.append((protein_id, cluster_id))
                    
                    if len(batch_data) >= chunk_size:
                        conn.executemany(
                            'INSERT OR REPLACE INTO protein_clusters VALUES (?, ?)',
                            batch_data
                        )
                        conn.commit()
                        batch_data = []
            
            # æ’å…¥å‰©ä½™æ•°æ®
            if batch_data:
                conn.executemany(
                    'INSERT OR REPLACE INTO protein_clusters VALUES (?, ?)',
                    batch_data
                )
                conn.commit()
        
        conn.close()
        logger.info("è›‹ç™½è´¨èšç±»æ˜ å°„åŠ è½½å®Œæˆ")
    
    def load_cluster_tree(self):
        """åŠ è½½èšç±»å±‚æ¬¡æ ‘æ•°æ®"""
        # ä¸‹è½½å¹¶è§£å‹èšç±»æ ‘æ–‡ä»¶
        gz_path = self.download_file(self.urls['clusters_tree'], 'clusters.tree.v12.0.txt.gz')
        txt_path = self.extract_gz_file(gz_path)
        
        logger.info("å¼€å§‹åŠ è½½èšç±»å±‚æ¬¡æ ‘...")
        conn = sqlite3.connect(self.db_path)
        
        chunk_size = 10000
        batch_data = []
        
        with open(txt_path, 'r', encoding='utf-8') as f:
            # è·³è¿‡å¤´éƒ¨
            next(f)
            
            for line in tqdm(f, desc="å¤„ç†èšç±»å±‚æ¬¡æ ‘"):
                parts = line.strip().split('\t')
                if len(parts) >= 3:
                    child_cluster_id = parts[0]
                    parent_cluster_id = parts[1]
                    distance = float(parts[2])
                    
                    batch_data.append((child_cluster_id, parent_cluster_id, distance))
                    
                    if len(batch_data) >= chunk_size:
                        conn.executemany(
                            'INSERT OR REPLACE INTO cluster_tree VALUES (?, ?, ?)',
                            batch_data
                        )
                        conn.commit()
                        batch_data = []
            
            # æ’å…¥å‰©ä½™æ•°æ®
            if batch_data:
                conn.executemany(
                    'INSERT OR REPLACE INTO cluster_tree VALUES (?, ?, ?)',
                    batch_data
                )
                conn.commit()
        
        conn.close()
        logger.info("èšç±»å±‚æ¬¡æ ‘åŠ è½½å®Œæˆ")
    
    def get_statistics(self) -> Dict:
        """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {}
        
        # è›‹ç™½è´¨æ€»æ•°
        cursor.execute('SELECT COUNT(*) FROM protein_info')
        stats['total_proteins'] = cursor.fetchone()[0]
        
        # ç‰©ç§æ•°é‡
        cursor.execute('SELECT COUNT(DISTINCT species_id) FROM protein_info')
        stats['total_species'] = cursor.fetchone()[0]
        
        # é«˜ç½®ä¿¡åº¦ç›¸äº’ä½œç”¨æ•°é‡
        cursor.execute('SELECT COUNT(*) FROM protein_interactions')
        stats['high_confidence_interactions'] = cursor.fetchone()[0]
        
        # è¯¦ç»†ç›¸äº’ä½œç”¨æ•°é‡
        cursor.execute('SELECT COUNT(*) FROM protein_interactions_detailed')
        stats['detailed_interactions'] = cursor.fetchone()[0]
        
        # æœ‰åºåˆ—çš„è›‹ç™½è´¨æ•°é‡
        cursor.execute('SELECT COUNT(*) FROM protein_sequences')
        stats['proteins_with_sequences'] = cursor.fetchone()[0]
        
        # èšç±»ç›¸å…³ç»Ÿè®¡
        cursor.execute('SELECT COUNT(*) FROM cluster_info')
        stats['total_clusters'] = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM protein_clusters')
        stats['protein_cluster_mappings'] = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM cluster_tree')
        stats['cluster_tree_edges'] = cursor.fetchone()[0]
        
        # å„ç‰©ç§çš„è›‹ç™½è´¨æ•°é‡ï¼ˆå‰10ï¼‰
        cursor.execute('''
            SELECT species_id, COUNT(*) as count 
            FROM protein_info 
            GROUP BY species_id 
            ORDER BY count DESC 
            LIMIT 10
        ''')
        stats['top_species'] = cursor.fetchall()
        
        # æœ€å¤§èšç±»çš„ä¿¡æ¯ï¼ˆå‰10ï¼‰
        cursor.execute('''
            SELECT cluster_id, cluster_size 
            FROM cluster_info 
            ORDER BY cluster_size DESC 
            LIMIT 10
        ''')
        stats['top_clusters'] = cursor.fetchall()
        
        conn.close()
        return stats
    
    def extract_all_data(self):
        """æ‰§è¡Œå®Œæ•´çš„æ•°æ®æå–æµç¨‹ - ä¸ºå±‚æ¬¡åŒ–ç‰¹å¾å»ºæ¨¡å‡†å¤‡æ•°æ®"""
        logger.info("å¼€å§‹STRINGæ•°æ®æå–æµç¨‹ï¼ˆå±‚æ¬¡åŒ–ç‰¹å¾å»ºæ¨¡ç‰ˆæœ¬ï¼‰...")
        
        # 1. è®¾ç½®æ•°æ®åº“
        self.setup_database()
        
        # 2. åŠ è½½åŸºç¡€è›‹ç™½è´¨ä¿¡æ¯
        self.load_protein_info()
        
        # 3. åŠ è½½è›‹ç™½è´¨åºåˆ—ï¼ˆç”¨äºPSSMç”Ÿæˆï¼‰
        self.load_protein_sequences()
        
        # 4. ç­›é€‰é«˜ç½®ä¿¡åº¦ç›¸äº’ä½œç”¨ï¼ˆåŸºç¡€ç‰ˆæœ¬ï¼‰
        self.filter_high_confidence_interactions()
        
        # 5. åŠ è½½è¯¦ç»†ç›¸äº’ä½œç”¨æ•°æ®ï¼ˆå¤šé€šé“è¯æ®è¯„åˆ†ï¼Œç”¨äºHGCNï¼‰
        self.load_detailed_interactions()
        
        # 6. åŠ è½½èšç±»ä¿¡æ¯ï¼ˆç”¨äºMoEä¸“å®¶æ¨¡å‹çš„å®¶æ—åˆ†ç±»ï¼‰
        self.load_cluster_info()
        
        # 7. åŠ è½½è›‹ç™½è´¨èšç±»æ˜ å°„
        self.load_protein_clusters()
        
        # 8. åŠ è½½èšç±»å±‚æ¬¡æ ‘ï¼ˆç”¨äºå±‚æ¬¡åŒ–å»ºæ¨¡ï¼‰
        self.load_cluster_tree()
        
        # 9. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        stats = self.get_statistics()
        logger.info("å±‚æ¬¡åŒ–ç‰¹å¾å»ºæ¨¡æ•°æ®æå–å®Œæˆï¼ç»Ÿè®¡ä¿¡æ¯:")
        for key, value in stats.items():
            logger.info("  %s: %s", key, value)
        
        return stats

def main():
    """ä¸»å‡½æ•°"""
    extractor = StringDataExtractor(confidence_threshold=0.95)
    stats = extractor.extract_all_data()
    
    print("\n" + "="*60)
    print("STRINGå±‚æ¬¡åŒ–ç‰¹å¾å»ºæ¨¡æ•°æ®æå–å®Œæˆï¼")
    print("="*60)
    
    print("\nğŸ“Š åŸºç¡€æ•°æ®ç»Ÿè®¡:")
    print(f"  æ€»è›‹ç™½è´¨æ•°é‡: {stats['total_proteins']:,}")
    print(f"  æ€»ç‰©ç§æ•°é‡: {stats['total_species']:,}")
    print(f"  æœ‰åºåˆ—çš„è›‹ç™½è´¨: {stats['proteins_with_sequences']:,}")
    
    print("\nğŸ”— ç›¸äº’ä½œç”¨ç½‘ç»œæ•°æ®:")
    print(f"  é«˜ç½®ä¿¡åº¦ç›¸äº’ä½œç”¨ï¼ˆåŸºç¡€ï¼‰: {stats['high_confidence_interactions']:,}")
    print(f"  è¯¦ç»†ç›¸äº’ä½œç”¨ï¼ˆå¤šé€šé“ï¼‰: {stats['detailed_interactions']:,}")
    
    print("\nğŸŒ³ å±‚æ¬¡èšç±»æ•°æ®ï¼ˆç”¨äºMoEä¸“å®¶æ¨¡å‹ï¼‰:")
    print(f"  æ€»èšç±»æ•°é‡: {stats['total_clusters']:,}")
    print(f"  è›‹ç™½è´¨-èšç±»æ˜ å°„: {stats['protein_cluster_mappings']:,}")
    print(f"  èšç±»å±‚æ¬¡æ ‘è¾¹æ•°: {stats['cluster_tree_edges']:,}")
    
    print("\nğŸ† å‰10ä¸ªç‰©ç§çš„è›‹ç™½è´¨æ•°é‡:")
    for species_id, count in stats['top_species']:
        print(f"  ç‰©ç§ {species_id}: {count:,} ä¸ªè›‹ç™½è´¨")
    
    print("\nğŸ” å‰10ä¸ªæœ€å¤§çš„èšç±»:")
    for cluster_id, size in stats['top_clusters']:
        print(f"  èšç±» {cluster_id}: {size:,} ä¸ªè›‹ç™½è´¨")
    
    print("\n" + "="*60)
    print("âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼Œæ”¯æŒä»¥ä¸‹å±‚æ¬¡åŒ–ç‰¹å¾å»ºæ¨¡ç»„ä»¶:")
    print("  1ï¸âƒ£ PSSMç”Ÿæˆ â† è›‹ç™½è´¨åºåˆ—")
    print("  2ï¸âƒ£ å°æ³¢å˜æ¢ â† PSSMçŸ©é˜µ")
    print("  3ï¸âƒ£ MoEä¸“å®¶æ¨¡å‹ â† èšç±»å®¶æ—ä¿¡æ¯")
    print("  4ï¸âƒ£ HGCNå›¾å­¦ä¹  â† å¤šé€šé“ç›¸äº’ä½œç”¨æ•°æ®")
    print("="*60)

if __name__ == "__main__":
    main()
